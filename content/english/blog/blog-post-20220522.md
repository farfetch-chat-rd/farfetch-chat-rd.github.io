---
title: "Why are time-series databases relevant for a Conversational AI platform?
"
date: 2022-05-22T16:45:01+01:00
author: Catarina Figueiredo
image : "images/blog/timeseries2022/timeseries.jpg"
bg_image: "images/featue-bg.jpg"
categories: ["Technology"]
tags: ["blog", "database", "conversational-commerce", "time-series"]
draft: false
type: "post"
---
## Summary

The Natural Language Understanding (NLU) is the first step when processing the user’s utterance. It performs both Slot Filling and Intent Classification creating a structured representation of the utterance.
This research focused on a deep-dive analysis of state-of-the-art Natural Language Understanding (NLU) models, and an analysis including two main groups of NLU models, namely:

- **Joint Intent Prediction & Slot-filling NLU**, where intent classification and slot filling are jointly optimized and learned. Here, we have analyzed both the JointBERT and Stack-propagation NLU models.

- **Context-aware NLU** that, differently from JointBERT and Stack-propagation, take as input not only the current user utterance but also the previous utterances as context. Here, we have analyzed both BERT-DST and SimpleTOD models. Although the original implementation of both BERT-DST and SimpleTOD models do not perform intent prediction, they can be easily extended for joint intent classification and slot-filling.

As the vast majority of the analyzed NLU models rely on the BERT architecture, this document starts by introducing the BERT model. Then, after describing all the considered NLU models, this document ends with a comparative table between them.

- [BERT background](#bert-background)
  * [Input & Output Representations](#input-output-bert)
  * [Pre-training BERT](#pre-training-bert)

    
## BERT background

[BERT](https://arxiv.org/pdf/1810.04805.pdf)’s model architecture is a multi-layer bidirectional Transformer encoder based on the original [Transformer model](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html). Accordingly, it consists of a stack of multiple identical blocks each containing a multi-head self-attention and a fully connected sub-layer with residual connections.

### Input & Output Representations <a name="input-output-bert"></a>

The input to BERT is a sequence of tokens (words), and the output is a sequence of vectors, one for each input token. To make BERT handle a variety of downstream tasks, the input representation of BERT is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence (see  **Figure 1)** .

<figure>
<p align="center">
<img src="/images/blog/NLU2022/BERT_input_format.png"  alt="BERT input format" width="650"/>
</p>
<figcaption align = "center"><b>Figure 1: BERT input format (from XX).</b></figcaption>
</figure>


The first token of every sequence is always a special classification token `[CLS]`, whose final hidden state is used as the aggregate sequence representation. The final hidden states of the other tokens are used as token-level representations.

For sentence pairs tasks (e.g., Q&A), sentence pairs are packed together into a single sequence, where tokens from the two input sentences are separated by another special token - `[SEP]`. Additionally, BERT’s input layer adds an additional segment embedding to differentiate tokens from the pair of sentences. For single text sentence tasks (e.g., sentiment classification), the input sequence also ends with the special `[SEP]` token.

Therefore, the input representation of a given token is constructed by summing the corresponding token, segment, and position embeddings (see  **Figure 2**).

<figure>
<p align="center">
<img src="/images/blog/NLU2022/BERT_input_representation.png"  alt="BERT input format" width="550"/>
</p>
<figcaption align = "center"><b>Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings (from XX).</b></figcaption>
</figure>

### Pre-training BERT <a name="pre-training-bert"></a>

To learn bidirectional contextualized representations and inter-sentence relationships, the BERT model is pre-trained 
on two unsupervised language modeling tasks, using the BooksCorpus and the English Wikipedia corpora, namely:

1. **Masked Language Modeling (masked LM)**: where the objective is to predict randomly masked tokens, 
by using the special `[MASK]` token, in the input sequence;

2. **Next Sentence Prediction (NSP)**: where the objective is to predict whether two input segments follow each other in 
the original text. Positive examples are created by taking consecutive sentences from the text corpus, 
whereas negative examples are created by picking segments from different documents.

The pre-trained BERT model provides powerful context-dependent sentence-level and token-level representations,
which can be used for a wide range of target-specific tasks (e.g., NER, Q&A, etc - see **Figure 3**), without substantial 
task-specific architecture modifications (i.e. with just extra projection layers and simple fine-tuning procedures).

<figure>
<p align="center">
<img src="/images/blog/NLU2022/BERT_finetuning.png"  alt="BERT input format" width="550"/>
</p>
<figcaption align = "center"><b>Figure 3: Overall pre-training and fine-tuning procedures for BERT (from XX).</b></figcaption>
</figure>

As described next, BERT can be easily extended to joint intent classification and slot filling (e.g., JointBERT) or
dialogue state tracking (e.g., BERT-DST).