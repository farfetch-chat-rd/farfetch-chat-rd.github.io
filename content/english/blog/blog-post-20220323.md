---
title: "iFetch Talking Series"
date: 2022-03-24T08:55:00+01:00
author: Ricardo G. Sousa
image : "images/blog/blog-conference-3.jpg"
bg_image: "images/featue-bg.jpg"
categories: ["Talks"]
tags: ["Conference", "seminars"]
draft: false
type: "post"
---

This week marks the start of a series of talks about the iFetch project that I’ll be giving. Our goal with iFetch is to create a trustworthy multimodal conversational agent for the online fashion marketplace. 


How do you get away from question-answering chatbots? What sets conversational agents apart from traditional chatbots? How can multimodality be used to address various customers’ intents? What should be the best policy to address our customer’s goals and be capable of responding in the right tone? How do you avoid sounding “robotic”?


Because our conversational agent comprises many workstreams, we’ve split the forthcoming talks into multiple sessions during this first semester. However, we recognize that not everyone in our audience will be able to attend all of the plenaries described below, so we’ll keep the sessions centered on specific topics relevant to the project on each of these sessions so you can pick the one that fits your interest the most.


## APRP - Associação Portuguesa de Reconhecimento de Padrões, 25th of March
This series will begin on the 25th of March, in Aveiro, for the APRP – Associação Portuguesa de Reconhecimento de Padrões. As an APRP member, I'm thrilled to be able to present iFetch at the first Spring Meeting. In this session, I'll explain why businesses are rapidly turning to virtual assistants backed by AI agents, as well as why and how iFetch proposes creating a new generation of task-oriented conversational agents that interact with users smoothly using verbal and visual data. One of the most critical obstacles is obtaining the appropriate and sufficient amount of data to handle all of the challenges. I'll go about the various issues of capturing and recognizing customer utterances, as well as how we deal with ambiguity. This session concludes with how we are representing our catalogue followed with some lessons learnt. 


## Priberam ML Lunch Seminars, 19th of April
At Priberam, we will discuss one of the pillars of a conversational system: context, building on the momentum of the previous session regarding the tools built by our team to have representative visual encodings. To accomplish this, we must first gain a deeper understanding of the many agent categories, as well as which strategy one should employ: neural approaches or template-based methods. Later on, we will share how updates to the conversation state need to reflect the key choices that were made in the previous turn and how one can take advantage of the improvements to neural models.


## Deep Learning Sessions Lisboa, 18th of May
Many of us are aware that nowadays multimodality is central. However, multimodal chatbots is a widely unexplored area, where users and the conversational agent communicate by natural language and visual data. In the Deep Learning Sessions Lisboa we will explore how one can engage the customer by defining iFetch as a fashion connoisseur that will enhance customer knowledge about the FARFETCH catalogue, its brands and partnerships.


## FARFETCH Tech Talks, tba
Later in this semester, I will wrap up this series with seminar talk at FARFETCH Tech Talks. Why and what business perspectives and customer pain points can be leveraged through conversational AI? How can we provide high-end clientele with the best in class online shopping experience to serve the massive paradigm shift into online conversational assistants while meeting the expectations set by high-end physical stores? 

In this talk we will explore how we are augmenting customer experience through accurate and reliable AI-enabled multimodal tools by advising and influencing customer shopping journey and how a state-of-the-art multimodal conversational framework can provide near real-time conversations between customers and FARFETCH.

## About the Speaker
[Ricardo Sousa](https://rjgsousa.github.io/) is Principal Data Scientist at FARFETCH. Driven to quickly trial novel Machine Learning approaches to disrupt different business areas, Ricardo was one key contributor in the "Search and Discovery" cluster. As a manager of Data Science, he co-lead four multidisciplinary product development teams composed of product, engineers, and scientists.

Ricardo is currently leading the conversational commerce initiatives at FARFETCH to disrupt the high-end fashion commerce industry and contribute to novel solutions with Conversational AI Agents.

With research interest in fields related to Information Retrieval leverage by modern Machine Learning, Computer Vision and Natural Language Processing he continues to contribute to the scientific community with articles in journals, conferences about his work at FARFETCH and co-organizing international events such as Multimodal Conversational AI Agents at ACM Multimedia.

Find more on his [homepage](https://rjgsousa.github.io/).


----
Blog photo by [John-Mark Smith](https://unsplash.com/@mrrrk_smith?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).
